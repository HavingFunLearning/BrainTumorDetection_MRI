{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob ## to grap the files easily \n",
    "import matplotlib.pyplot  as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 ## help to read image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we need to adjust the dimensions, and also following the conventions (R,G B channels order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  plot_random(healthy,tumor, num = 5):\n",
    "    healthy_imgs = healthy[np.random.choice(healthy.shape[0],num,replace=False)]\n",
    "    tumor_imgs = tumor[np.random.choice(tumor.shape[0],num,replace=False)]\n",
    "\n",
    "    plt.figure(figsize=(16,9))\n",
    "    for i in range(num): ## matplotlib starts from 1 differently from when using python\n",
    "        plt.subplot(1,num, i+1) # 1 row with 5 columns, i want to allocate the neext image in the position i+1\n",
    "        plt.title(\"healthy\")\n",
    "        plt.imshow(healthy_imgs[i])\n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    for i in range(num): ## matplotlib starts from 1 differently from when using python\n",
    "        plt.subplot(1,num, i+1) # 1 row with 5 columns, i want to allocate the neext image in the position i+1\n",
    "        plt.title(\"tumors\")\n",
    "        plt.imshow(tumor_imgs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Torch Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the abstract definition of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to implement getitem() and len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI(Dataset):\n",
    "    def __init__(self,scores):\n",
    "        self.x = scores\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [12,3,7,7]\n",
    "d = MRI(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basically, the get_item method is the one invocked automatically when i use the []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the same is done when using len(d) for the method __len__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually the same is done when i use + "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here since i need to extract the features i use the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        tumor = []\n",
    "        healthy = []\n",
    "        # cv2 - It reads in BGR format by default\n",
    "        for f in glob.iglob(\"./brain_tumor_dataset/yes/*.jpg\"):\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.resize(img,(128,128)) # I can add this later in the boot-camp for more adventure\n",
    "            b, g, r = cv2.split(img)\n",
    "            img = cv2.merge([r,g,b])\n",
    "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1])) # otherwise the shape will be (h,w,#channels)\n",
    "            tumor.append(img)\n",
    "\n",
    "        for f in glob.iglob(\"./brain_tumor_dataset/no/*.jpg\"):\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.resize(img,(128,128)) \n",
    "            b, g, r = cv2.split(img)\n",
    "            img = cv2.merge([r,g,b])\n",
    "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
    "            healthy.append(img)\n",
    "\n",
    "        # our images\n",
    "        tumor = np.array(tumor,dtype=np.float32)\n",
    "        healthy = np.array(healthy,dtype=np.float32)\n",
    "        \n",
    "        # our labels\n",
    "        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)\n",
    "        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)\n",
    "        \n",
    "        # Concatenates\n",
    "        self.images = np.concatenate((tumor, healthy), axis=0)\n",
    "        self.labels = np.concatenate((tumor_label, healthy_label))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {'image': self.images[index], 'label':self.labels[index]}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    ## These are gray scale images so from 0 to 255\n",
    "    ## Usually we normaliza either from -1 to 1 or from 0 to 1\n",
    "    ## here i do the second option\n",
    "    def normalize(self):\n",
    "        self.images = self.images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri = MRI()\n",
    "mri.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's print from the data loader using squeeze to remove one dimension ans reshape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way of iterating\n",
    "names={0:'Heathy', 1:'Tumor'}\n",
    "dataloader = DataLoader(mri, shuffle=True)\n",
    "for i, sample in enumerate(dataloader):\n",
    "    img = sample['image'].squeeze()\n",
    "    img = img.reshape((img.shape[1], img.shape[2], img.shape[0]))\n",
    "    plt.title(names[sample['label'].item()])\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "        nn.Tanh(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=5),\n",
    "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "        nn.Tanh(),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=5))\n",
    "        \n",
    "        self.fc_model = nn.Sequential(\n",
    "        nn.Linear(in_features=256, out_features=120),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=120, out_features=84),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=84, out_features=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_model(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "x = x.reshape((2,2,2,2)) # look at this, in the following way: 2 data points composed of 2 2x2 matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0) ## this take the length of the fist dimension, in this case 2 since we have 2 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\n",
    "v = v.reshape((2,2,2,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.view(-1) # this just flatten everything even more complex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.size(0), -1) # this is saying to flatten independently each data point --> so given 2 data points i get 2 flattened vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do i need to use x.view?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the code, we have the batch of data for the training/testing so doing this I flatten each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking about Traning and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the model.eval() for 2 reasons:\n",
    "- to turn off the dropout at test time\n",
    "- to not use the batch normalization for each test batch (just use the mean and variance learnt using the training)\n",
    "\n",
    "An important remark: when using .eval() i don't turn off the gradient computation! it just changes the forward method behaviour!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "### When creating a tensor i need to send to the right device! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4853, 0.6339, 0.7948, 0.7593, 0.3901, 0.1631, 0.5840, 0.9849, 0.0156,\n",
      "        0.0392]) torch.float32 <class 'torch.Tensor'> torch.FloatTensor\n",
      "tensor([0.4853, 0.6339, 0.7948, 0.7593, 0.3901, 0.1631, 0.5840, 0.9849, 0.0156,\n",
      "        0.0392]) torch.float32 <class 'torch.Tensor'> torch.FloatTensor\n",
      "tensor([2.3555e-01, 4.0182e-01, 6.3174e-01, 5.7653e-01, 1.5217e-01, 2.6611e-02,\n",
      "        3.4111e-01, 9.7003e-01, 2.4209e-04, 1.5342e-03])\n"
     ]
    }
   ],
   "source": [
    "# device will be 'cuda' if a GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# creating a CPU tensor\n",
    "cpu_tensor = torch.rand(10).to(device)\n",
    "# moving same tensor to GPU\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "\n",
    "print(cpu_tensor, cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type())\n",
    "print(gpu_tensor, gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type())\n",
    "\n",
    "print(cpu_tensor*gpu_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now this works since both are on the same kind of memory since no gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".detach() is used to remove the computation graph and retrieve the np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and .cpu() returns a copy in the cpu memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.cpu().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_dataset = MRI()\n",
    "mri_dataset.normalize()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn_model): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=5, padding=0)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): AvgPool2d(kernel_size=2, stride=5, padding=0)\n",
       "  )\n",
       "  (fc_model): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=84, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.0001\n",
    "EPOCH = 400\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "dataloader = DataLoader(mri_dataset, batch_size=32, shuffle=True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCH):\n",
    "    losses = []\n",
    "    for D in dataloader:\n",
    "        optimizer.zero_grad() ## Remove the cached gradients \n",
    "        data = D['image'].to(device)\n",
    "        label = D['label'].to(device)\n",
    "        y_hat = model(data)\n",
    "        # define loss function\n",
    "        error = nn.BCELoss() \n",
    "        loss = torch.sum(error(y_hat.squeeze(), label))\n",
    "        loss.backward() ## This computes the gradients\n",
    "        optimizer.step() ## This update the weights\n",
    "        losses.append(loss.item())\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch+1, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
